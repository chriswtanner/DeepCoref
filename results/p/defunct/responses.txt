First, I sincerely thank you very much, as it’s clear you spent time and effort to thoroughly understand and review the paper!

However, I wish to clarify an important issue which seems to be a misunderstanding regarding your “Based on the text […]” and "Similarly, figure 3 [...]" comments (weakness #1 and #2): since our works concerns two models which operate in a serial manner, I realize the wording can be really tricky.

In short, we were very careful to never test on training data.  The corpus has 43 topics (aka directories).  The CCNN (1st model) was trained on mention-pairs from topics 1-17, then it made mention-pair predictions for topics 18-23 (e.g., mention7 co-ref'ing with mention9 = 0.41).  Next, the Neural Cluster model uses these predicted mention-pair scores in topics 18-20 as training data to learn how to cluster, and it makes clustering predictions for topics 21-23 (aka the official 'dev set'), per Figure 3's top line of #s.  We experimented with different features for the CCNN model, and the feature set that yielded the best results (per the Neural Clustering model's performance on the 'dev set' topics 21-23) was to use only "Lemma" (see Paper Line #642).  Since Lemma gave the best results on the Dev Set, we used only that feature for when we ran our final, complete system.  That is, from scratch, we now trained the CCNN on topics 1-20 (to be consistent with others' previous works), using only the Lemma feature, and the CCNN gave mention-pair predictions for topics 21-43.  We used its predictions of topics 21-23 as training data for the Neural Clustering model.  Last, the Neural Clustering model clustered the mentions from topics 24-43.  The test mentions in topics 24-43 could have been defined based on either (1) gold mention boundaries; (2) the mentions used by HDDCRP; or (3) the mentions used by Choubey.  Table 2 shows results on all 3 different mention boundaries.  The gold mentions case yielded 81.2 F1.  We did not tune on the test set.  Had we done so, we would have reported 81.5 F1 in Table 2, since that is performance had we used Lemma+Character feature set, but since that did not yield the highest score on the dev set, we did not use it.  We only report all features on all sets (in Figure 3) to illustrate this, and to show the trends of which features offer which performance.

In the last paragraph of Section 5.2, we followed the same setup as I just described.  That is, when we considered adjusting the train/dev sizes, I meant for the CCNN (e.g., train = topics 1-10; dev = 11-17), and the final system was still blind to the test set and was only tuned on the performance on the dev set.  Same is true for cross-validation experiments, wherein we trained the CCNN on topics 1-17 but held one out each time, so as to have a larger dev set that would serve as input to the Neural Clustering Model.  We ignored these 2 other approaches because they offered slightly worse performance on the dev set mentions (topics 21-23), they in turn still yielded great performance on the test set (still much better than the competitors' 72-75).

As for cross-document results, I agree, and I wished to include them, too. Our cross-doc model is an exact copy of our within-doc model (just w/ different parameters and feature-set, per the dev set scores).  However, describing the details (e.g., performing WD first, or iteratively, and how the models work together), plus results, exceeded the 8-page limit, and I was already at the limit.  If our paper is chosen for submission, the additional page would allow space for such.  In short, our cross-document model yields results of 

As for the code release, that was purely accidental as I did not understand we were allowed to submit code during the submission, as I thought it might reveal authorship. I apologize.  Per the chair's recommendation, I have now provided the code under an anonymous Google Drive: 
