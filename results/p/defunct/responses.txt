First, thank you very much, as it’s clear you spent time and effort to thoroughly understand and review the paper!

However, I wish to clarify an important issue which seems to be a misunderstanding regarding your “Based on the text […]” and "Similarly, figure 3 […]" comments (weakness #1 and #2): since our works concerns two models which operate in a serial manner, I realize the wording can be tricky.

In short, we were very careful to never test on training data.  The corpus has 43 topics (aka directories).  The CCNN (1st model) was trained on mention-pairs from topics 1-17, then it made mention-pair predictions for topics 18-23 (e.g., mention7 co-ref'ing with mention9 = 0.41).  Next, the Neural Cluster model uses these predicted mention-pair scores in topics 18-20 as training data to learn how to cluster, and it makes clustering predictions for topics 21-23 (aka the official 'dev set'), per Figure 3's top line of #s.  We experimented with different features for the CCNN model, and the feature set that yielded the best results (per the Neural Clustering model's performance on the 'dev set' topics 21-23) was to use only "Lemma" (see Paper Line #642).  Since Lemma gave the best results on the Dev Set, we used only that feature for when we ran our final, complete system.  That is, from scratch, we now trained the CCNN on topics 1-20 (to be consistent with others' previous works), using only the Lemma feature, and the CCNN gave mention-pair predictions for topics 21-43.  We used its predictions of topics 21-23 as training data for the Neural Clustering model.  Last, the Neural Clustering model clustered the mentions from topics 24-43.  The test mentions in topics 24-43 could have been defined based on either (1) gold mention boundaries; (2) the mentions used by HDDCRP; or (3) the mentions used by Choubey.  Table 2 shows results on all 3 different mention boundaries.  The gold mentions case yielded 81.2 F1.  We did not tune on the test set.  Had we done so, we would have reported 81.5 F1 in Table 2, since that is performance had we used Lemma+Character feature set, but since that did not yield the highest score on the dev set, we did not use it.  We only report all features on all sets (in Figure 3) to illustrate this, and to show the trends of which features offer which performance.

In the last paragraph of Section 5.2, we followed the same setup as I just described.  That is, when we considered adjusting the train/dev sizes, I meant for the CCNN (e.g., train = topics 1-10; dev = 11-17), and the final system was still blind to the test set and was only tuned on the performance on the dev set.  Same is true for cross-validation experiments, wherein we trained the CCNN on topics 1-17 but held one out each time, so as to have a larger dev set that would serve as input to the Neural Clustering Model.  We ignored these 2 other approaches because they offered slightly worse performance on the dev set mentions (topics 21-23), they in turn still yielded great performance on the test set (still much better than the competitors' 72-75).

As for cross-document results, I agree, and I wished to include them, but we ran out of paper space. Our cross-doc model is an exact copy of our within-doc model.  Amazingly, when evaluating against the HDDCRP's test mention, we achieve a CoNLL F1 score of 59.0 (outperforming their system), and if we use gold mention boundaries for testing, we achieve 68.6.  If our paper is chosen for submission, we could detail this in the additional page that is granted.

I agree with you about Figure 2; I originally drew such but wasn't sure which way would be clearer.

As for the code release, that was purely accidental as I did not understand we were allowed to submit code during the submission, as I thought it might reveal authorship. I apologize.  Per the chair's recommendation, I have now provided the code under an anonymous Google Drive: https://drive.google.com/open?id=1UOkQcTz2bvVzTNr6s2wjVuKrStg_BoiW


-----
First, thank you very much, as it’s clear you spent time and effort to thoroughly understand and review the paper.

However, there might be a misunderstanding.  I did not make this clear in the paper, but for training, our system used the same mentions as identified in the HDDCRP paper -- they use a SRL system then filter all of the mentions per their own rules.  Only during test time did we evaluate our system against (1) the gold test mentions; (2) HDDCRP's test mentions; (3) Choubey's attempt of HDDCRP's.

We chose not to make our own Event Detection system due to the scope of that task, as it is a separate line of research.  Although, in the future, we may pursue such.

As for cross-document results, I agree, and I wished to include them, too, but we ran out of paper space. Our cross-doc model is an exact copy of our within-doc model.  Amazingly, when evaluating against the HDDCRP's test mention, we achieve a CoNLL F1 score of 59.0 (outperforming their system), and if we use gold mention boundaries for testing, we achieve 68.6.  If our paper is chosen for submission, we could detail this in the additional page that is granted.
-----
First, thanks for the thorough read and feedback!

Our FFNN Model's inference time was a good bit faster than Agglomerative Clustering because FFNN is a simple feed-forward network, so training it only happened once; however w/ Agglomerative, during dev time, we would search for the optimal threshold point, which involved running agglomerative several times.  This could be optimized I suppose, but in general both of these were pretty quick methods, running in just a few minutes total.

We ran out of paper space to mention this, but when we run our same model for the task of cross-document, we also outperform HDDCRP with 59.0.  When evaluating against gold test mentions, we achieve 68.6

-----
- Response to reviewers’ comments on task and application:
I ran out of paper space, but the cross-doc results should/could be included.  Our model, when run against HDDCRP's test mentions yields 59.0 (outperforming their system), and if we test against the test sets's gold mention boundaries, we achieve 68.6

- Response to reviewers’ comments on software / systems:
Sorry for not linking directly to the code.  I uploaded it here, on this anonymous Google Drive: https://drive.google.com/open?id=1UOkQcTz2bvVzTNr6s2wjVuKrStg_BoiW 
-----
Reviewer #1 seemed to misunderstand/misread an important part that is crucial for any sound research, and thus appropriately gave a poor rating based on this misunderstanding.  That is, they seemed to think that we tested on a trained set of data?  Since we have 2 models that run in a serial-fashion, the language for describing such is complicated.  I addressed these possible misunderstanding in this rebuttal, so hopefully it clarifies it.

