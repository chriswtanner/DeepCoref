%
% File naaclhlt2018.tex
%
%% Based on the style files for NAACL-HLT 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2018}
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{url}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\aclfinalcopy % Uncomment this line for the final submission
\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Towards Featureless Event Coreference Resolution via Conjoined Convolutional Networks}

\author{Chris Tanner \textnormal{and} Eugene Charniak\\
Brown Linguistic Laboratory of Information Processing \\
  Brown University \\
  Providence, RI  02912 \\
  {\tt \{christanner,ec\}@cs.brown.edu} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
% motivation / others' weaknesses
Coreference resolution systems for entities and/or events almost always make use of many linguistic, parsing-based features.  In contrast, (1) we introduce a new state-of-the-art event coreference resolution system which uses only lemmatization and its corresponding precomputed word-/char- embeddings, achieving 67.2 CoNLL F1 score on a common ECB+ test set, along with setting a new baseline of 8X.XX for the test set at large. (2) We exhaustively illustrate the performance of other commonly-used features.  The crux of our system is that it first makes pairwise event-coreference predictions by using a Siamese Convolutional Neural Network (henceforth referred to as Conjoined Convolutional Neural Network or CCNN).  Last, (3) we cluster the event mentions with a simple, but novel, neural approach which performs merges in an easy-first, cluster-holistic manner, allowing our system to be less susceptible to errors that are made exclusively from min-pairwise decisions.
\end{abstract}

\section{Introduction}
% problem definition/statement
Coreference resolution is the task of trying to identify -- within a single text or across multiple documents -- which \textit{mentions} refer to the same underlying discourse \textit{entity} or \textit{event}.  Naturally, one may be solely interesting in determining if two given entities co-refer to the same object (e.g., a pairwise prediction of \textit{she} and \textit{Mary} co-referring); however, ultimately, coreference resolution is a clustering task, whereby we wish to group all like-mentions together.  Successfully doing so can be useful for several other core NLP tasks that concern natural language understanding, such as information extraction \cite{Humphreys:1997}, topic detection \cite{Allan:1998}, text summarization \cite{Daniel:2003}, knowledge base population \cite{Cross_Document_Coreference_Resolution_A_Key_Technology_for_Learning_by_Reading}, question answering \cite{Narayanan:2004:QAB:1220355.1220455}, etc.
% motivation
Coreference Resolution has always been one of the fundamental tasks within NLP, and with the ever-increasing amount of textual, digital data that is generated and consumed in present-day, it remains both important and challenging.

% problem definition
Specifically, coreference systems aim to find a globally-optimal fit of mentions to clusters, whereby every mention $m$ in the corpus is assigned to exactly one cluster $C$, the membership of which constitutes that every ${m_i,m_j} \in C_k$ is co-referent with each other.  If a given $m_i$ is not anaphoric with any other $m_j$, then it should belong to its own $C_k$ with a membership of one.  Further, the number of distinct clusters is not known apriori but is bounded by the number of mentions and is part of the system's inference.  Finding such a globally-optimal assignment is NP-Hard and thus computationally intractable.  In attempt to avoid this, systems typically perform pairwise-mention predictions, then use those predictions to build up clusters. The specific modelling strategies for such approximately fall into two categories: (1) mention-ranking / mention-pairs; and (2) entity/event-level.

\textbf{Mention-ranking} models define a scoring a function $f(m_i,m_j)$ which operates on any $m_j$ and possible antecedent $m_i$, where $m_i$ occurs earlier in the document and could be null (represented by $\epsilon$ and denoting that $m_j$ is non-anaphoric).  Although these models are by definition less expressive than entity/event-level models, their inference can be relatively simple and effective, allowing them to be fast and scalable.  As a consequence, they have often been the approach used by many state-of-the-art systems \cite{Soon:2001:MLA:972597.972602,DBLP:conf/emnlp/DurrettK13,DBLP:journals/corr/WisemanRS16}.

\textbf{Mention-pair} models are defined almost identically, with the subtle difference being the target objective of the pairwise-candidates.  That is, mention-ranking model aim to find the ideal $m_i$ antecedent for every $m_j$, whereas mention-pair models score all possible $(m_i,m_j)$ pairs \cite{Bengtson:2008:UVF:1613715.1613756,Soon:2001:MLA:972597.972602}.

\textbf{Entity/Event-level} models differ in that they focus on building a global representation of each underlying entity or event, the basis of which determines each mention's membership -- as opposed to the most local pairwise- elements that comprise the aforementioned models \cite{DBLP:journals/corr/WisemanRS16,clark2016improving}.

In this work, we use a novel and powerfully simple mention-ranking model that is designed solely to discriminate between pairs of input features: Siamese Convolutional Neural Networks, which, for political reasons, we will henceforth refer to as our newly-coined term, Conjoined Convolutional Neural Networks (or CCNN).  Further, we aim to replace the main weakness of mention-ranking models with an approach resembling the main strength of entity/event-level models.  Specifically, we aim to combine all linked mention pairs into a cluster via a simple neural, easy-first, clustering approach which factors in a small, but effective, notion of the entire cluster at large.
%instead of combining all linked pairs of mentions into a cluster, independent of how they relate to the other mentions in the cluster
%in lieu of sim
%maintaining a true entity-level representation against which all candidate mentions will be compared and added, we  


Additionally, a common theme of coreference research is that systems typically use a plethora of relatively-expensive parsing-based features, including dependency parse information, lemmatization, WordNet hypernyms/synonyms, FrameNet semantic roles, part-of-speech, etc.  Although some research includes a listing of the learned feature weights of a system (corresponding to each feature's importance) \cite{journals/tacl/YangCF15}, there has been a striking lack of work which takes the minimalistic approach and illustrates the effects of using few features.  We aim to address this by starting with the widely-accepted strong baseline of \textit{SameLemma} -- two objects are co-referent if, and only if, they have the same lemmatization -- and then evaluate the effectiveness of slowly adding other commonly used features.

Finally, in general, \textit{entity} coreference resolution has received drastically more attention than \textit{event} coreference.  This lack of research could in part be due to events' often involving more complex nature: a single underlying event may be described via multiple lexicographically-differing mentions, yet different underlying events may also be represented by mentions that lexicographically look the same.  This latter case is less common in entity coreference; other than pronouns, usually mentions' having the same text is a strong indication that the mentions are co-referent.  In this paper, we are exclusively interested in event coreference.

In summary, we introduce a novel approach to event coreference resolution by performing mention-ranking with a Conjoined Convolutional Neural Network and unusually few features.  We contribute a detailed performance analysis of other commonly used features.  And last, we combine our predicted mention pairs into a cluster via a simple, neural approach which attempts to represent each cluster as a whole, yielding us with state-of-the-art results on the ECB+ corpus.


% the problem
%In this paper,   Further, we wish to address the possible shortcoming of 
%Again, the main shortcoming of the two former mention-* models is that after making pairwise decisions, combining them into clusters only requires satisfying local constraints
% our solution
% explain subsequent chapters

\section{Related Work}
% initial start
Event coreference resolution has received significantly less attention than its entity-based counterpart.  The seminal research on event-based coreference can be traced back to the DARPA-initiated MUC conferences, whereby the focus was on limited scenarios involving terrorist attacks, plane crashes, management succession, resignation, etc.  Most notable from this period were the works by Humphreys et al. \shortcite{Humphreys:1997} and Bagga and Baldwin \shortcite{Bagga:1999:CEC:1608810.1608812}, which applied event coreference to the tasks of information extraction, topic detection and tracking.  The successor of MUC was the annual ACE program, which addressed more fine-grained events with the aforementioned challenging situations wherein mentions may have identical text shared by many distinct events.


%Event coreference resolution has received significantly less attention than its entity-based counterpart.  The seminal research on event-based coreference can be traced back to the DARPA-initiated MUC conferences, whereby the focus was on limited scenarios involving terrorist attacks, plane crashes, management succession, resignation, etc.  Most notable from this period were the works by Humphreys et al. \shortcite{Humphreys:1997} and Bagga and Baldwin \shortcite{Bagga:1999:CEC:1608810.1608812}, which applied event coreference to the tasks of information extraction, topic detection and tracking.

% continued efforts
%The successor of MUC was the annual ACE program, which occurred from 1999 to 2008 and furthered researched by including corpora containing more fine-grained events.  This addressed the aforementioned challenge whereby events may be an overloaded term in that multiple, uniquely distinct underlying events may use textually-identical mention representation.  One of the most notable papers from this period, which also illustrates the varied modelling approaches, is the graph-based system by Chen and Ji \shortcite{Chen:2010:GCC:1870490.1870491}.  This is an example of a mention-pair model, as they constructed a graph from the fully-complete weight matrix that corresponds to all mention pairs.

In present day, Deep Learning is revolutionarily affecting NLP; however, there has been a few successful applications of deep learning to coreference resolution, almost all of which have been for entity-based system.  We attribute this dearth to the fact that coreference resolution is inherently a clustering task, which tends to be a non-obvious modality for deep learning.  We divide the work relevant to ours into two categories: (1) deep learning approaches; and (2) the systems which use the same ECB+ corpus \cite{ECB+} as we do.

\subsection{Deep Learning Approaches}
To the best of our knowledge, there are only five other publications which apply deep learning to coreference resolution, four of which focus on entity coreference.

Sam Wiseman, et. al. built mention-ranking models \shortcite{DBLP:conf/acl/WisemanRSW15,DBLP:conf/naacl/WisemanRS16} which are trained with a heuristic loss functions that assign different costs based on the types of errors made, and their latter work used mention-ranking predictions towards an entity-level model via LSTM hidden states \cite{Hochreiter:1997:LSM:1246443.1246450}.

Separately, Clark and Manning \shortcite{clark2016improving,DBLP:journals/corr/ClarkM16a} also built both a mention-ranking and an entity-level model, the former of which was novel in using reinforcement learning to find the optimal loss values for the same four distinct error types defined in Wiseman's, et. al \shortcite{DBLP:conf/acl/WisemanRSW15} work.

\subsection{Systems using ECB+ Corpus}
% most related
For our research, we make use of the ECB+ corpus \cite{ECB+}, an extension of EventCorefBank (ECB) \cite{Bejan:2010:UEC:1858681.1858824}, which we further describe in Section \ref{sec:corpus}.  In short, this rich corpus provides annotations for both entities and events, yet most research chooses to focus on using \textit{either} events or entities, not both.  To the best of our knowledge, there are only two papers which focus on the event mentions of ECB+: The Hierarchical Distance-dependent Chinese Restaurant Process (HDDCRP) model by Yang, et. al. \shortcite{journals/tacl/YangCF15} and Choubey's and Huang's Iteratively-Unfolding approach \shortcite{Choubey2017EventCR}.   Consequently both are highly relevant to our work.

\subsubsection{HDDCRP Model}
\label{sec:HDDCRP}
% HDDCRP
Yang, et. al's HDDCRP model \shortcite{journals/tacl/YangCF15} uses a clever and inspiring mention-pair approach, whereby they first use logistic regression to train the set of parameters $\theta$ for the similarity function in Equation \ref{eq:hddcrp}.  
\begin{equation}
\label{eq:hddcrp}
f_{\theta}(x_i,x_j) \propto \textnormal{exp\{} \theta^T\psi(m_i,m_j)\textnormal{\}}
\end{equation}
Then, the crux of their system is that in a Chinese-restaurant-process fashion, they probabilistically assign links between mentions purely based on the scores provided by this similarity function.  That is, the value emitted by $f(m_i,m_j)$ is directly correlated with the probability of $(m_i,m_j)$ being chosen as a linked pair.  However, identical to Bengtson's and Roth's work \cite{Bengtson:2008:UVF:1613715.1613756}, the HDDCRP model then automatically forms clusters by tracing through all linked pairs; all mentions that are reachable by a continuous path become assigned the same cluster.  This hinges on the transitive property holding true for coreference.  For example, if ${(m_1,m_3),(m_3,m_5)}$ and $(m_5,m_6)$ are each individually linked via the scoring function, then a cluster $C_i$ is formed, where $C_i = \{m_1,m_3,m_5,m_6\}$, even though $(m_1,m_5)$ or $(m_3,m_6)$ may have had very low similarity scores.  After these initial clusters are formed for both within-doc (WD) and cross-document (CD) mentions, their system continues to perform Gibbs sampling until convergence, allowing mentions to freely shift between other clusters according to the similarity function.  We aim to improve this shortcoming, as detailed in Section \ref{sec:clustering}.

\subsubsection{Neural Iteratively-Unfolding Model}
\label{sec:Choubey}
% Choubey
Most recently, Choubey and Huang \shortcite{Choubey2017EventCR} introduced the first neural model that is exclusive for event coreference.  Their system also fits into the mention-pair paradigm, whereby mentions are predicted by a feed-forward neural network. For within-doc predictions, their network features are primarily based on the cosine similarity and euclidean distance of input-pair embeddings.  The cross-document model is identical, other than adding context features, too.  This was an important finding, for they assert that when using the ECB+ corpus, within-doc coreference did not benefit from using mention context.  That is, the mention words themselves were sufficient.  Similar to the weakness of the HDDCRP model, they form clusters based on local mention-pair predictions, independent of mentions' relevance to the cluster at large.

\section{System Overview}
\subsection{Mention Identification}
\label{sec:mentionid}
Coreference systems are predicated upon having entity/event mentions identified.  In fact, this identification process is the focus of a different line of research: entity recognition and event detection are the names given to identifying entities and events, respectively.  This separation of tasks allows coreference systems to be evaluated precisely on their ability to link/cluster together appropriate mentions.  Thus, it is common practice for coreference systems to either: (1) use gold mentions that are defined by the true annotations in the corpus, or (2) use an off-the-shelf entity recognition or semantic role labelling system (SRL).  We do both.  That is, the majority of our results are shown with having used gold mentions.  Yet, it was critically important to us to ensure we developed a competitive system, so it was imperative to use the same mentions that were used in the two aforementioned system that focus on events of the ECB+ corpus.  The HDDCRP model set the precedence by using an SRL system to predict mentions, then they pre-processed and filtered many of those, yielding their system with an imperfect but reasonable set of mentions that shares a moderate overlap with the gold mentions.  Determining the exact mentions that were used by HDDCRP was one of the most challenging and time-consuming processes of our research.

Naturally, Choubey's, et. al. system also aimed to use their same mentions.  After numerous exchanges with the author, it was clear that their set of mentions was similar and reasonable for research, but understandably not the same as that used by HDDCRP.  Namely, they filtered out: (1) all predicted mentions which were not in the gold set (false positives), and (2) predicted mentions which were singletons (ones that did not cluster with a mention from another document).

We evaluate our systems having used the: (1) gold mentions; (2) HDDCRP-predicted mentions; (3) Choubey-predicted mentioned.

\subsection{Reproducibility}
As illustrated, reproducing coreference results can be naturally tedious, as it is challenging to ensure every token identified and parsed according to one system perfectly aligns and is represented correctly by another.  Since these issues comprised a large amount of our research efforts, we aim to ameliorate the situation by providing our code online, which is easily runnable on any of the aforementioned sets of mentions and evaluations.  Additionally, our code runs in just a few minutes on a single Titan X GPU.
\subsection{Models}
Our system is comprised of two neural models:
\begin{itemize}
  \item Conjoined Convolutional Neural Network -- used for making mention-pair predictions.  (Section \ref{sec:CCNN})
  \item Neural Clustering -- uses the pairwise predictions to cluster mentions into events (Section \ref{sec:clustering})
\end{itemize}
\subsection{Corpus}
\label{sec:corpus}
We exclusively make use of the ECB+ corpus \cite{ECB+}, which is the largest available dataset with annotations for event coreference.  The corpus is comprised of 43 distinct \textit{topics} -- categories or news stories.  Each of the 43 topics has 2 sub-topics which are similar in nature but distinctly different from each other.  For example, Topic 1 contains 2 sub-topics, 1 of which about Lindsay Lohan checking into a rehab center in Malibu, California, and the other about Tara Reid checking into a rehab center in the same city.  Each sub-topic contains roughly 8-15 short text documents which all concern the same given sub-topic.  Following the convention of the aforementioned researchers who use this corpus, we divide the corpus into the following splits: training set contains topics 1-20; dev set contains topics 21-23, and the test set contains topics 24-43.  For those interested in this corpus, note that the actual structure of the corpus files happens to not include a topic directory named \#15 or \#17, so the listed divisions correspond to the sequential ordering of topics and size of each split, not the exact structure of directory names.

Corpus statistics are listed in Table \ref{tab:ECB1}, where it is clear that the majority of gold mentions are one token in length (e.g, \textit{announced}).

\begin{table}
\centering
\begin{tabular}{c|c|c|c|c|}
\cline{2-5}
& Train & Dev & Test & Total \\ \cline{1-5} \hline
\multicolumn{1}{ |c| }{\# Documents} & 462 & 73 & 447 & 982   \\ %\cline{1-5}
\multicolumn{1}{ |c| }{\# Sentences} & 7,294 & 649 & 7,867 & 15,810    \\ %\cline{1-5}
\multicolumn{1}{ |c| }{\# 1-Token Mentions} & 1,938 & 386 & 2,837 & 5,161    \\ %\cline{1-5}
\multicolumn{1}{ |c| }{\# 2-Token Mentions} & 142 & 52 & 240 & 434    \\ %\cline{1-5}
\multicolumn{1}{ |c| }{\# 3-Token Mentions} & 18 & -- & 25 & 43    \\% \cline{1-5}
\multicolumn{1}{ |c| }{\# 4-Token Mentions} & 6 & -- & 7 & 13   \\ \cline{1-5}
\end{tabular}
\caption{Statistics of the ECB+ Corpus}
\label{tab:ECB1}
\end{table}

\section{Conjoined Convolutional Neural Network}
\label{sec:CCNN}
\subsection{Motivation}
As a recap, there has yet to exist a deep learning event-level model for event coreference resolution.  Although it is tempting to explore this option due to the success of deep learning entity-based models, we were motivated for a few reasons to develop a model that fits the mention-pair paradigm: not only has the previous work on the ECB+ corpus shown strength from using mention-pair models, but analyzing the training set data illustrates that most golden co-referent clusters contain little variance in the lemma-representation of each mention (see Figure \ref{fig:lemmaPower}).  Since there is such high homogeneity on an intra-cluster level, it suggests that entity-level representations might not offer much benefit.  Naturally, one could argue that intra-cluster representation is not conclusive evidence; that is, the context of mentions, which could differ drastically for each co-referent mention, might be beneficial.  However, as Choubey, et. al. \shortcite{Choubey2017EventCR} concluded, context seems to offer not benefit at all for within-doc coreference on the ECB+ corpus.  Thus, we are interested in developing a powerful pairwise-prediction model, such as a Conjoined Neural Network \cite{SiameseNet}.
\begin{figure}
  \caption{A boat.}
  \label{fig:lemmaPower}
\end{figure}
\subsection{Overview}
Conjoined Neural Networks (or Siamese Networks, as they are known as) were first introduced by Bromly and LeCun \shortcite{SiameseNet} towards a task whereby the goal was to accurately determine if two input items (hand signatures) were in fact of the same class or not.  Specifically, a Conjoined Network can be defined as twin neural networks, each of which accepts distinct inputs, but they are eventually joined by a loss function over their highest-level features.  The loss function computes a metric that represents the similarity between the two input pairs (e.g., euclidean distance, cosine similarity, hamming distance, etc).  The two networks are said to be conjoined because they share the same weights and thus work together as one network that learns how to discriminate.  The benefits of tying the weights are that it: (1) ensures that similar inputs into each network will be mapped accordingly, otherwise, they could be mapped to hidden representations that are disproportionately dissimilar from their input representations; and (2) forces the network to be symmetric.  Namely, if we were to abstractly view the Conjoined Network as a function, then:

\vspace{4mm}

 $CCNN(f_i,f_j) \equiv CCNN(f_j,f_i)$

\vspace{4mm}

This is critical, as the CCNN should yield the same similarity score independent of the ordering of its input pair.

Last, we posit that CCNN's have been shown to perform well in low-resource situation \cite{Koch2015SiameseNN}.  This is ideal for our task, as it is highly likely that at test time we will encounter event mentions that are OOV.  We desire our model to discriminately learn the relationships of input mentions, rather than exclusively relying on and memorizing the input values themselves.

As for the choice of Conjoined Network, Convolutional Neural Networks (CNNs) have recently proven to be highly useful for many tasks in NLP, including sentence classification \cite{DBLP:conf/emnlp/Kim14}, machine translation \cite{DBLP:conf/acl/GehringAGD17}, dependency parsing \cite{DBLP:journals/corr/YuV17}, etc.  Likewise, we choose to use a CNN due to their power in learning sub-regions of features, and the relations thereof -- rather than heavily relying manually-defined features.

\subsection{Input Features}
\label{sec:features}
Since our CCNN needs each mention to be represented exclusively by its own input, we used none of the relational features that are common in other coreference systems (e.g., SameLemma, Jaccard Similarity of the mentions' context words, First common WordNet parent, \# of Sentence in between Mentions, etc).  We thoroughly tested the following input features and their listed variants:
\begin{itemize}
  \item \textbf{Part-of-Speech:} 1-hot representation; LSTM-learned embeddings after maping the entire corpus to their POS tags
  \item \textbf{Lemmatization}: 300-length word embeddings for the lemma of each mention token, where the embedding came from running GLoVE \cite{pennington2014glove} either on our corpus, or using their provided pre-trained 6 billion and 840 billion token crawls.
  \item \textbf{Dependency Lemma:} we use the dependent parent and/or children of each mention token, and we represent it via the aforementioned lemma embeddings)
  \item \textbf{Character Embeddings:} we represent each mention token as a concatenation of its character embeddings (truncated or padded up to the first 20 characters), where we experimented with character embeddings being either (1) random 20-length embeddings or (2) pre-trained 20-length embeddings
  \item \textbf{Word Embeddings:} same as the embeddings listed for \textit{lemma}, just we apply these embeddings for the word tokens themselves, not the lemma of each token.
\end{itemize}
Note, since mentions are of varying token length (see Table \ref{tab:ECB1}), we need a convention to standardize the vector-length (e.g., to 300 dimension).  We experimented with summing across all token embeddings in place, averaging, and concatenated to a particular $N$-length size.  For clarity, \textit{averaging} for a given mention $m$'s embedding is calculated via:

$m_{emb}[i] = \frac{\sum_{t \in T}t_{emb}[i]}{|T|}$, where $t$ is a token in the set of tokens $T$ that comprise $m$

\subsection{Architecture}
We define the embedding for a given token $t$, as: $t_{emb} = t_{f_{1}} \oplus t_{f_{2}} \oplus \ldots \oplus t_{f_{n}},$

where $\oplus$ represents vector concatenation and $t_{f_{i}}$ represents a specific input feature vector for token $t$.

Naturally, we may want to convolve over the context of mention $m$, too, by including the $N$ words before and after $m$.  Thus, our input for mention $m$ is a matrix $M$, and a la Kim \shortcite{DBLP:conf/emnlp/Kim14}, we zero-pad any tokens that would be beyond our window.

\vspace{3mm}

Let $\textbf{M}$ represent the full matrix corresponding to mention $m$: $\textbf{M} \in \mathbb{R}^{(2*N+1) \times d}$

\vspace{1mm}

and

\vspace{1mm}

$\textbf{M}_{(i,j),(k:l)}$ represent the sub-matrix of $M$ from $(i,j)$ to $(k,l)$.

\vspace{3mm}

We define a kernel/filter with dimensions $(h,w)$, where $h < (2*N+1)$ and $w < d$.  This allows the kernel to operate on sub-sections of the embeddings.  The kernel has an associated weight matrix $\textbf{w} \in \mathbb{R}^{w \times h}$.  Therefore, starting at a given index $(i,j)$ within mention matrix $\textbf{M}$, a feature $c_{i}$ is defined as:
\begin{equation}
c_{i} = f(\textbf{w}^{T}\textbf{M}_{(i:i+h-1),(j:j+w-1)} + b)
\end{equation}
where $b \in \mathbb{R}$ is an added bias term.  The kernel runs over every possible sub-section of mention matrix $\textbf{M}$, yielded a feature map $\textbf{c} \in \mathbb{R}^{(2*N-h) \times (d-w-1)}$

\vspace{3mm}

We use 64 kernels (we experimented with all powers of 2, from 2 - 256), along with ReLU as our activation function.

Dropout is then applied (we experimented with values from 0 to 0.5).

Next, we repeat this processing by adding convolution and dropout again, then apply max-pooling to get a single $\hat{c} = max\{\textbf{c\}} \in \mathbb{R}$.

Last, we apply dropout again, then merge all of our kernels to feed into a final ReLU.

\subsection{Loss}
The goal of our model is to maximize discriminability between mentions representing different events, while enforcing features to be as similar as possible when they are of the same event.  Contrastive Loss is perfectly suited for this objective \cite{SchroffKP15,pmlr-v48-liud16}, as shown in Equation \ref{eq:contrastive}.  Because our input is mention pairs, there is a strong class imbalance, so we down-sample the negative examples, yielding a training set of 5 negative examples per positive example.
\begin{equation}
\begin{aligned}
L(&\hat{y},y)=\frac{1}{2N}\sum_{n=1}^{N}[(y)d^2 \\
&\qquad {} + (1-y)*(max(1-d,0))^2] \\
&\textnormal{where }d=\|a_{n}-b_{n}\|_{2}
\end{aligned}
\label{eq:contrastive}
\end{equation}

\subsection{Optimization}
We used Adagrad for optimization.  We also experimented with RMS and found the results to be effectively identical.

\section{Neural Clustering}
\label{sec:clustering}
In mention-pair models, it is common practice to first assign a probabilistic score to every mention-pair, and then to cluster with a different model.

\subsection{Existing Clustering Approaches}
\textbf{Agglomerative Clustering} is a simple but effective approach.  Namely, it first treats each mention as being its own singleton-cluster.  Then, it repeatedly merges two distinct clusters into a new cluster, whereby the two selected clusters are the ones which respectively contain the two mentions which are the shortest-distance pair.  Although this is a strong baseline, as seen in Yang, et. al. \shortcite{journals/tacl/YangCF15}, there are three main weaknesses:
\begin{enumerate}
\item One must introduce a stopping heuristic; otherwise, merging will continue until there is just 1 cluster.
\item If one uses a threshold value $\alpha$ to stop (i.e., stop merging when the shortest pair exceeds $\alpha$), this asserts the data is uniform, yet the distribution of pairwise predictions inevitable differs from document-to-document.  That is, for one document, there may be a clear division where $\alpha = 0.5$ separates the pairs nicely, but in another document, the values inherently are lower or higher, causing $0.5$ to be inappropriate.
\item Most significant, each cluster merge is based solely on two individual mentions, yet these mentions may not be representative of the cluster at large.
\end{enumerate}

\textbf{HDDCRP and Iterative-Folding Clustering} both contain the issue \#3 above, as detailed in Sections \ref{sec:HDDCRP} and \ref{sec:Choubey}.

\subsection{Our Approach}
We aim to use the strengths of agglomerative clustering, while replacing the shortcomings.  We train a classifier to learn the most likely {positive cluster merge}, where the cluster is represented more holistically than a single pair.  Instead of operating on a mention-to-mention basis to dictate the cluster merges, we consider every possible mention-to-cluster.

Specifically, denoting a mention as $m$, and a cluster as $C$, we learn a function $f(m_i,C_y)$ that predicts the likelihood of an appropriate, positive merging of $(C_x,C_y)$, where $m_i \in C_x$ and $C_x \neq C_y$.

Let $d(m_i,m_j)$ be the mention-pair distance score emitted by our CCNN model, while $m_i \in C_x$, and $m_j \in C_y$.  Function $f(m_i,C_y)$ is based on four simple features:
\begin{itemize}
  \item min-pair distance: $d(m_i,m_j)$
  \item avg-pair distance: $\frac{\sum_{m_j \in C_y} d(m_i,m_j)}{\|C_y\|}$
  \item max-pair distance: $d(m_i,m_j)$
  \item size of candidate cluster: $\|C_y\|$
\end{itemize}

The first three features serve to better represent the cluster at large.  For example, if a given $m_i$ were evaluated against two other clusters $C_1$ and $C_2$, it might be the case that both clusters of have the same min-pair distance score.  Yet, the \textit{distribution} of distance scores with the other mentions in each cluster might shed light onto which cluster has more similar mentions.  We experimented with including the full distribution of distance scores, along with the variance in distance scores, but we received the best results from the min, avg, max distances -- probably because most golden clusters contain just 1-4 mentions.  The last feature (cluster size) serves is an explicit measure to help prevent clusters from growing too large.

At testing time, we evaluate every $(m_i, C_y)$ pair.  

We define $f$ as a feed-forward neural network with 1 hidden layer of 25 units.  We use ReLU activation without dropout, and our model predicts the probability of a positive merge via a 2-class softmax function.  Our loss function is weighted binary cross-entropy, to account for the class imbalance situation that most mentions should not be merged with most clusters.  We optimize with Adagrad.  Like agglomerative, at each iteration, we merge only the $(m_i,C_y)$ pair that yielded the highest score (likelihood of a positive merge).  We continue to merge until model no longer predicts a merge (when all candidate pairs are $< 0.5$).  Thus, unlike the aforementioned models, we do not require an additional parameter for our stopping criterion.

Our pseudocode is shown in Algorithm \ref{neural_cluster}.

\begin{algorithm}
    \begin{algorithmic}[1]
       \Require $n \geq 0$
    \end{algorithmic}
\caption{Neural Clustering}
\label{neural_cluster}
\end{algorithm}

Note, since our neural classifier requires as training data the mention-pair scores predicted by CCNN, our model is limited to using the dev set's scores as training data.  We also considered (1) adjusting the train/dev set sizes, yielding more dev data on which our clustering model could train; and (2) cross-validation.  Yet, the original train/dev/test split performed the best, signifying that it is most important for the CCNN model to have as much training data as possible.

\section{Results}
We evaluated against the gold test mentions which were identified in ECB+.

Since our model requires representing each mention independently of the other candidate mention, our features could not contain relational-type information (e.g., SameLemma, \# of sentences between Mentions, etc)\footnote{Although, we did experiment with extending our CCNN model by adding relational features as a merged-layer at the highest neural level.  However, doing so provided no benefit.}.  We experimented with the 5 features detailed in Section \ref{sec:features}: POS Embeddings, Lemma Embeddings, Dependency Lemma Embeddings, Character Embeddings, Word Embeddings.

Most coreference systems include dozens of features; however, in the interest of understanding which features were most useful, we built-up our system in an additive manner: we started with just one feature, testing each feature independently.  If features seemed to help, we considered combining them with others.  The results from all feature combinations are illustrated in Table \ref{tab:allfeatures}.

\textbf{Lemma Embeddings} were the most useful feature, followed closely by Character Embeddings.  Since \textit{SameLemma} historically has been a strong, difficult baseline, this is unsurprising.  Using the embeddings of the lemmas, especially with the power of the CCNN, provides much more expression than the mere boolean feature of \textit{SameLemma}.  Our best results for this feature came from using the pre-trained 840 billion-token GloVe embeddings.

\textbf{Character Embeddings} were effective for the same reason String Edit Distance is often a strong feature.  It is particularly useful for our corpus because the gold mentions for a given cluster are usually textually very similar to one another, if not identical.  Both random character embeddings and pre-trained ones yielded the same performance, suggesting that the power comes from the uniqueness of characters, not any \textit{meaning} conveyed in the characters.

Combining Lemma Embeddings with Character Embeddings yielded \textbf{our highest performance -- 8X.X CoNLL F1 score.}  As this is the first event coreference system to test on the ECB+ golden test mentions, we denote this score as the new baseline to which to compare future systems.

It is clear that Lemma + Character Embeddings are complementary; the semantic information conveyed within the lemma embeddings complement the syntactic information of character embeddings.  Related, POS by itself was a poor feature.  However, combining POS with either Lemma Embeddings or Character Embeddings offered strong results.  Interestingly, the Iteratively-Unfolding model used Lemma + POS \shortcite{Choubey2017EventCR}.

Ideally, a classifier should precisely learn how to combine all features smartly, such that the unhelpful features are given 0 weight.  However, in practice, that is often extremely difficult, due to both the large parameter space and the high entropy wherein some combinations of features seem to equally help as much as hurt.  Thus, we conclude that one should try to use the fewest features as possible for coreference resolution, then to expand appropriately.

\subsection{Comparison to Other Systems}
\textbf{HDDCRP} chose to not use the gold test mentions provided by ECB+.  Instead, they used a semantic role labeller to predict and filter all mentions.
To compare against their system, we spent an enormous effort working with the HDDCRP source code and data to ensure we accurately use their exact predicted mentions.  Of the 3,290 gold mentions in ECB+, their system failed to find/use 676 of them.  Of their 3,109 predicted mentions, 495 were false positives and in fact not actual gold mentions.  Since their system uses these imperfect mentions, yet tests the coreference performance against the gold mentions, their system encounters a steep performance loss by default.  With complete confidence, we fairly compare our system with theirs by using the same, imperfectly predicted mentions.  We outperform their system on this exact test set, as shown in Table \ref{tab:allResults}.

\textbf{Iteratively-Unfolding} attempted to use the same predicted test mentions as HDDCRP.  As mentioned in Section \ref{sec:mentionid}, they removed false positives and singleton-mentions which belong to events that are only found within the given document (no cross-doc references).  After numerous correspondences, we compare our system against the same set of test mentions they used, and the results are also shown in Table \ref{tab:allResults}.

\section*{Conclusion}
% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{naaclhlt2018}
\bibliography{naaclhlt2018}
\bibliographystyle{acl_natbib}

\appendix

\section{Supplemental Material}
\label{sec:supplemental}
Submissions may include resources (software and/or data) used in in the work and described in the paper. Papers that are submitted with accompanying software and/or data may receive additional credit toward the overall evaluation score, and the potential impact of the software and data will be taken into account when making the acceptance/rejection decisions. Any accompanying software and/or data should include licenses and documentation of research review as appropriate.


NAACL-HLT 2018 also encourages the submission of supplementary material to report preprocessing decisions, model parameters, and other details necessary for the replication of the experiments reported in the paper. Seemingly small preprocessing decisions can sometimes make a large difference in performance, so it is crucial to record such decisions to precisely characterize state-of-the-art methods. 

Nonetheless, supplementary material should be supplementary (rather
than central) to the paper. {\bf Submissions that misuse the supplementary 
material may be rejected without review.}
Essentially, supplementary material may include explanations or details
of proofs or derivations that do not fit into the paper, lists of
features or feature templates, sample inputs and outputs for a system,
pseudo-code or source code, and data. (Source code and data should
be separate uploads, rather than part of the paper).

The paper should not rely on the supplementary material: while the paper
may refer to and cite the supplementary material and the supplementary material will be available to the
reviewers, they will not be asked to review the
supplementary material.


Appendices ({\em i.e.} supplementary material in the form of proofs, tables,
or pseudo-code) should come after the references, as shown here. Use
\verb|\appendix| before any appendix section to switch the section
numbering over to letters.

\section{Multiple Appendices}
\dots can be gotten by using more than one section. We hope you won't
need that.

\end{document}
